% Final year project - Final report
% Author: Alexander Jeffery

\documentclass{article}

% Package imports:
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{newalg}

\DeclareGraphicsExtensions{.png}

\begin{document}

\begin{titlepage}
    \center
    \textsc{\LARGE University of Sussex} \\
    \vspace{16mm}
    \textsc{\Large Department of Informatics} \\
    \vspace{4mm}
    \textsc{\large Computer Science BSc - Final Year Project} \\
    \vspace{8mm}
    \rule{\linewidth}{0.5mm}\\
    \vspace{2mm}
    {\huge \bfseries An Interpreter and Compiler for a Functional Programming Language}
    \rule{\linewidth}{0.5mm}
    \begin{minipage}{0.4\textwidth}
        \vspace{10mm}
        \begin{flushleft}
            \emph{Author:} \\
            Alexander \textsc{Jeffery} \\
            Candidate No: \textsc{18512}
        \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \vspace{10mm}
        \begin{flushright}
            \emph{Supervisor:} \\
            Dr Martin F. \textsc{Berger}
        \end{flushright}
    \end{minipage} \\
    \vspace{10mm}
    {\large \today}
    \vfill
\end{titlepage}

\newpage\mbox{}\newpage

\section*{Statement of Originality}
This report is submitted as part requirement for the degree of Computer Science BSc at the University of Sussex. It is the product of my own labour except where indicated in the text. The report may be freely copied and distributed provided the source is acknowledged.
\\\\
\vspace{20mm}
{\large Signed:}
\\\vspace{20mm}
{\large Alexander \textsc{Jeffery}}
\pagebreak

\section*{Summary}
Programming languages are divided into two main groups: imperative and functional. Imperative languages are derived from computer architecture, whereas functional languages are derived from the lambda calculus.
\\\indent The lambda calculus is a mathematical calculus used to represent computation, via the process of syntactic substitution. It serves as a theoretical foundation for functional programming languages.
\\\indent The aim of this project was to implement a functional programming language, using the lambda calculus as a theoretical foundation. It was implemented using the Haskell programming language, which itself is functional.
\\\indent The main features implemented are an interpreter and a compiler, for a language with type inference, polymorphism, lists, function literals, and list, sum and product types.
\\\indent The interpreter and compiler were implemented successfully, however the compiled implementation uses a somewhat naive implementation strategy.
\pagebreak

\tableofcontents
\pagebreak

\section{Introduction}

Imperative programming languages are those in which programs consist of a sequence of steps that modify a state, i.e. values stored in computer memory. The sequence of steps taken may depend on this state. The imperative principle of sequential execution and state change is ultimately derived from the nature of computer architecture, which is imperative in nature. Such languages have become increasingly sophisticated over many years, abstracting the computer's hardware, so that programmers need not concern themselves with understanding the intricate details of the computer, which are often irrelevant to the problem they are solving. Imperative languages do not abstract sequential execution (by definition).
\\
\indent In contrast, Functional programming languages are those in which programs consist of a set of mathematical equations, the evaluation of which constitutes computation. They are derived from the lambda calculus, rather than computer architecture, and are indeed an abstraction of the imperative nature of computer architecture.

\subsection{Purpose of this Report}
This report will explain in detail how computation is achieved in a functional programming language. It will give details of the implementation of a functional programming language, in the form of an interpreter and a compiler. The report will discuss development methodology and give a summary of the source code. It will provide an overview of the produced language, and conclude with a critical evaluation.

\pagebreak
\section{Background}

\subsection{Programming Language Implementation}
Even those unfamiliar with computers may have heard it said that 'computers only understand binary'. In a very specific sense, this is true - CPUs take their instructions in binary form, as voltages on a wire. Therefore we must be able to \emph{express} our computer programs in binary form (such a form is typically referred to as \emph{machine code}). This does not mean, however, that we must write our computer programs in machine code - modern computer programs are created with programming languages, which describe programs in a more abstract way. Typically, \emph{source code} is written in a particular programming language, and is then translated into an executable form via a process known as \emph{compilation}. A compiler is a computer program that takes source code as its input, and outputs an executable machine code file. Not all programming languages are implemented via compilation, however. Some programming languages are implemented via \emph{interpretation}, and some languages use a hybrid system of compilation and interpretation, usually to improve performance. The implementation of a typical programming language can be decomposed into several phases: 
\begin{enumerate}
    \item Lexical Analysis
    \item Syntax Analysis
    \item Semantic Analysis
    \item Optimisation
    \item Execution System
\end{enumerate}

\subsubsection{Lexical Analysis}
Lexical analysis is concerned with the identification of words in source code. The input to a lexical analyser, or \emph{lexer}, is a string of characters that (ideally) represents a program. The lexical analyser has the job of \emph{tokenising}, that is, breaking up the words in the string into \emph{tokens}, which are the 'atoms' of the programming language, the simplest units of meaning within the language. The output of a lexical analyser is a list of tokens, assuming all the words in the source code file are valid words in the programming language. If not, this is usually indicated to the user and no further work is done by the compiler.

\subsubsection{Syntax Analysis}
Syntax analysis is concerned with deriving structure from from a sequence of tokens. A syntax analyser, or \emph{parser}, has the job of building an \emph{abstract syntax tree} (AST). An AST is an abstract data structure which represents the structure of a computer program. They are generated from a list of tokens that are the output of the lexer, based on a description of the \emph{grammar} of the language, which is usually represented formally using a \emph{context-free grammar}. An AST, representing the computer program, is the output of the parser, assuming that the tokens that are input can be structured correctly (the correctness of any given structure is specified by the language's context-free grammar). If not, this is usually indicated to the user and no further work is done by the compiler.
\\\indent The figures below illustrate how some source code might be transformed into a list of tokens by a lexical analyser, and the list of tokens into an abstract syntax tree by a parser.
\\\\
\centerline{\textbf{Source Code - Factorial Calculation in C}}
\\\\
\indent \hspace{34mm} \texttt{int factorial(int n) \{    } \\
\indent \hspace{40mm} \texttt{    int val = 1;           } \\
\indent \hspace{40mm} \texttt{    while (n > 0) \{       } \\
\indent \hspace{48mm} \texttt{        val *= n;          } \\
\indent \hspace{48mm} \texttt{        n -= 1;            } \\
\indent \hspace{40mm} \texttt{    \}                     } \\
\indent \hspace{40mm} \texttt{    return val;            } \\
\indent \hspace{34mm} \texttt{\}                         } \\
\\
\centerline{\textbf{Token List - Factorial Calculation in C}}
\\\\
\centerline{\includegraphics[width=400px]{lexed}}
\\\\
\centerline{\textbf{Abstract Syntax Tree - Factorial Calculation in C}}
\\\\
\centerline{\includegraphics[width=500px]{parsed}}

\subsubsection{Semantic Analysis}
Semantic Analysis is concerned with ensuring that the semantics of AST, which is output by the parser, are correct. This typically includes \emph{type checking}, which ensures that the programmer made no \emph{type errors} in their program, for example, adding a variable consisting of a number, to a variable consisting of a string of characters. Semantic analysis may also include type \emph{inference}, depending on the programming language. This involves not only type checking, but determining what types variables have without the programmer having to write them down. Semantic analysis will also typically check for errors such as the use of an undefined variable, or the use of a variable prior to its definition.

\subsubsection{Optimisation}
Optimisation is concerned with improving performance of the program. At this point the program has been lexically, syntactically and semantically analysed, we have assurance that the program is well-formed, so the focus is no longer on determining the validity of the program, but rather on its performance. Optimisation takes on many different forms, at many different domains. A typical optimisation might be to generate some \emph{intermediate code} that represents the program at a level that is \emph{closer to the machine}, that looks much more like actual machine code. This intermediate code is then optimised using techniques such as register allocation optimisation, AST transformation and algebraic simplification. Such techniques are beyond the scope of this project, however. The optimised intermediate code can then be translated into machine code.
\\\indent The optimisation phase is often skipped with interpreted programming languages, since the focus of interpreted languages tend not to be on performance - often the focus is on portability or some other aspect related to the convenience of the programmer or user.

\subsubsection{Execution Systems}
\paragraph{Interpretation \\}
Interpretation is an execution system that performs \emph{immediate evaluation} of ASTs. Instead of generating machine code to be run at a later time, the program is immediately executed, having just been lexically, syntactically and semantically analysed. An interpreter is a meta-program (a program that operates on other programs) that takes a program as input, and returns the output of that program as its output.

\paragraph{Compilation \\}
Compilation involves outputting machine code while traversing the AST. The generated machine code is then saved as a file, which can be directly executed by the computer at a later time. Thus, we have a distinction between \emph{compile-time} and \emph{runtime}, that does not exist with interpretation. A compiler as a meta-program that takes a program as its input, and returns a semantically equivalent program, in a different programming language, as its output.

\paragraph{Just-In-Time Systems \\}
Just-in-time (JIT) compilation is similar to interpretation in that there is no compile-time/runtime distinction. However, machine code \emph{is} generated with JIT, but instead of it being written to a file, it is mapped directly into the computer's memory for immediate execution. A JIT system will have a program managing the generation and execution of machine code, so that it is as efficient as possible - for example, if some piece of source code is never executed, it would be wasteful to generate machine code for it. For this reason, source code (or an AST, depending on the strategy used by the JIT system) is only translated into machine code at the last possible moment prior to execution (hence the term just-in-time). JIT systems are effective because much more information about the execution of the program is known at runtime as opposed to compile-time, meaning that JIT-compiled code can be optimised to a much greater degree than code compiled ahead of time.

\paragraph{Hybrid Systems \\}
There are many hybrid systems in existence that combine interpretation, code generation and JIT systems into a programming language implementation. One such example is the Java programming language, which generates intermediate code at compile-time, and at runtime, the intermediate code is executed by a \emph{tracing} JIT system, where code is initially interpreted, and when a part of the program is determined to be 'hot code' - code that is executed frequently - it is translated to machine code and directly executed. The machine code that is run is then analysed and optimised further.
\\\indent There are a vast array of different hybrid systems in existence, and many possible combinations that have not been attempted. The goal is usually to maximise performance.

\subsection{Functional and Imperative}
The first functional language, LISP, first appeared in 1958, and has been a strong influence in modern functional languages, such as Haskell, ML and Clojure. Dialects of LISP, such as Common Lisp and Scheme are still in use today.
\\\indent The lambda calculus (often written '$\lambda$-calculus') is a mathematical calculus that is used to represent computations, and is Turing-complete, meaning that it can represent anything that can possibly be computed. It was developed by the mathematician Alonzo Church in 1936 as a proposed solution to the foundational crisis of mathematics. While it was not successful as a foundation for mathematics, it has proved effective as a foundation for functional programming languages. Since the lambda calculus is Turing-complete, it can even be used to \emph{simulate} an imperative programming language.
\\\indent The lambda calculus employs a single rule in order to achieve computation, known as $\beta$-reduction ('$\beta$' pronounced as 'beta'), whereby \emph{variables} are replaced with \emph{expressions} that are supplied as \emph{arguments} to \emph{functions}. More precisely, we say that a function is \emph{applied} to its arguments, whereby all occurrences of the parameterised variable in the function body are replaced with the argument expression. For example, consider a function, in a conventional programming language, that takes a single integer argument, and calculates the square of that argument, returning the result. In a C-like language, we might have:
\\\\
\indent \texttt{int square ( int x ) \{ return x * x ; \}}
\\\\
We might invoke this function using:
\\\\
\indent \texttt{square ( 10 ) ;}
\\\\
In the lambda calculus, we would say that the function \texttt{square} is applied to the argument, \texttt{10}. When a function such as this is applied to an argument in the lambda calculus, we think of the parameter \texttt{x} taken by \texttt{square} as being \emph{syntactically replaced} in the body of \texttt{square}, by the expression that is supplied as an argument. In this example, such a syntactic replacement would yield
\\\\
\indent \texttt{10 * 10}
\\\\
in the body of \texttt{square}. Such a syntactic replacement can be thought of as the mode of computation for the lambda calculus. That is, the lambda calculus achieves computation via this syntactic replacement.
\\
\indent Now let us consider the \texttt{square} function in lambda notation. We have:
\[ \lambda x.x * x \]
Immediately we see that the function name, \texttt{square}, is no longer present. This is because functions in the lambda calculus do not have names. This means that in order to use the function, we must write it down in its entirety, rather than refer to it by name like we did in the earlier example. Here, the '$\lambda$' symbol indicates that the expression that follows is a function, and that the '$x$' that follows indicates that $x$ is a parameter to the function. The $\lambda x$ is said to \emph{bind} the variable $x$. The '$.$' is there to delimit this binding of $x$ from the function body, which is the '$x * x$' part. We can apply (invoke) this function to the argument \texttt{10} as we did with the imperative example. In the lambda calculus, we write:
\[ (\lambda x.x * x)(10) \]
Here, the $10$ parameterises $x$. When we evaluate the expression, we are left with the body of the function, where every occurrence of $x$ is replaced with $10$:
\[ 10 * 10 \]
This syntactic replacement ($\beta$-reduction), is the sole method of computation in the lambda calculus. The formal syntax of the lambda calculus is given by the context-free grammar:
\[ M \:\; ::= \;\; x \;\; | \;\; \lambda x.M \;\; | \;\;  MN \]
where $x$ is an arbitrary variable name and $M$ and $N$ are arbitrary expressions. Closure properties for the lambda calculus are formalised as:
\[ M \rightarrow N \Rightarrow LM \rightarrow LN. \]
\[ M \rightarrow N \Rightarrow ML \rightarrow NL. \]
\[ M \rightarrow N \Rightarrow \lambda x.M \rightarrow \lambda x.N \]
Informally, we can describe the closure properties: if $M$ reduces to $N$, then $LM$ and $ML$ reduce to $LN$ and $NL$ respectively, where L is an arbitrary expression. $\beta$-reduction is formalised as:
\[ (\lambda x.M)N \rightarrow_\beta M[N \mapsto x] \]
\indent As stated previously, functional programming languages are derived from the lambda calculus, and so their method of computation is this $\beta$-reduction, rather than sequential execution. While they introduce extra syntax for convenience, all functional programs can be converted into lambda expressions that can be described by the given context-free grammar.

\subsection{Evaluation Strategies}
An important topic in the lambda calculus and functional programming is that of \textbf{evaluation strategy}. Since a lambda term may have many possible $\beta$-equivalent terms reachable in a single $\beta$-reduction, implementations must have rules, or strategies, to determine the order in which these reductions will be made. The most notable schemes are call-by-value, call-by-name and call-by-need.

\subsubsection{Call-By-Value}
With a call-by-value evaluation strategy, we evaluate arguments to functions before we substitute them into function bodies. Generally speaking, we always take the innermost, rightmost possible reduction. Formally, we have the following reduction rules (small-step semantics):
\begin{align*}
            eval_{CBV} ( (\lambda x.M) (N O) ) &= (\lambda x.M) (eval_{CBV} (N O) ) \\
    eval_{CBV} ( (\lambda x.M) (\lambda y.N) ) &= M[x \mapsto \lambda y.N]          \\
              eval_{CBV} ( (\lambda x.M) y )   &= M[x \mapsto y]
\end{align*}
\subsubsection{Call-By-Name}
With a call-by-name evaluation strategy, we substitute arguments into functions before we evaluate the arguments. Generally speaking, we always take the outermost, rightmost possible reduction. Formally, we have the following reduction rules (small-step semantics):
\begin{align*}
            eval_{CBN} ( (\lambda x.M) (N O) ) &= M[x \mapsto (N O)]       \\
    eval_{CBN} ( (\lambda x.M) (\lambda y.N) ) &= M[x \mapsto \lambda y.N] \\
              eval_{CBN} ( (\lambda x.M) y )   &= M[x \mapsto y]
\end{align*}
\subsubsection{Confluence and Normalisation}
The lambda calculus has the property of \textbf{confluence} (Church; Rosser, 1936: \emph{"Some properties of conversion"}). Informally, this means that if any term has a normal form, then it has only one normal form. Formally, we can say that if some term $M$ can be reduced to a term $M_a$ in any number of $\beta$-reductions (if $M \rightarrow^{*}_{\beta} M_a$), and can also be reduced to a term $M_b$ in any number of $\beta$-reductions (if $M \rightarrow^{*}_{\beta} M_b$) under another evaluation strategy, then there exists some term $M_c$ such that $M_a$ and $M_b$ can both be reduced to $M_c$ in any number of $\beta$-reductions ($M_a \rightarrow^{*}_{\beta} M_c$ and $M_b \rightarrow^{*}_{\beta} M_c$).
\\\indent The lambda calculus is also has the property of \textbf{weak normalisation}. This means that some, but not all terms have a normal form. (This is as opposed to strong normalisation, where every term has a normal form - the simply typed lambda calculus is strongly normalising, but the pure lambda calculus discussed here is only weakly normalising.) Shown below is an example of a term with no normal form.
\[ (\lambda x.xx)(\lambda x.xx) \]
By showing the reduction of this term, we can see that it has no normal form:
\[ (\lambda x.xx)(\lambda x.xx) \hspace{4mm} \rightarrow \hspace{4mm} xx[x \mapsto (\lambda x.xx)] \hspace{4mm} \rightarrow \hspace{4mm} (\lambda x.xx)(\lambda x.xx) \]
There are some terms, however, that have a normal form, but some reduction sequences of that term are non-terminating - consider the term:
\[ (\lambda x.\lambda y.y)((\lambda x.xx)(\lambda x.xx)) \]
Under a call-by-value strategy, we begin by reducing the right-hand term, which we have already seen is not normalising, and as a result, we do not find the normal form. If we use call-by-name, however, we first substitute the argument on the right-hand side into the function on the left-hand side:
\[ (\lambda x.\lambda y.y)((\lambda x.xx)(\lambda x.xx)) \hspace{4mm} \rightarrow \hspace{4mm} (\lambda y.y)[x \mapsto (\lambda x.xx)(\lambda x.xx)] \hspace{4mm} \rightarrow \hspace{4mm} \lambda y.y \]
Here we are left with a normal form. It is the case that for any term with a normal form, a call-by-name strategy is guaranteed to find that normal form, whereas with a call-by-value strategy, this is not guaranteed (as we have seen). For this reason, call-by-name evaluation may be a more desirable evaluation strategy than call-by-value, for a functional programming language.
\\\indent A downside to using call-by-name as the evaluation strategy for a functional programming language is that it can cause duplication of work. Consider the term:
\[ (\lambda x.x \hspace{1mm} x \hspace{1mm} x)M \]
where $M$ is some large term that requires many reduction steps to reach its normal form. Using call-by-value evaluation, we only have to evaluate this term once, the result of which is substituted into the function on the left. Using call-by-name evaluation, we substitute this term into the function three times, and triple the number of reduction steps required to reduce the entire term.
\subsubsection{Call-By-Need}
Call-by-need evaluation (sometimes called \emph{lazy} evaluation) is a variation on call-by-name. The semantics of call-by-need are difficult to formalise, but informally, arguments are evaluated after substitution, but where a term would be duplicated, it is \emph{shared}. In practice, this usually means that multiple pointers are kept to it, one for each occurrence. This strategy has the advantage of call-by-name that the normal form is guaranteed to be found if it exists, and the advantage of call-by-value that we need evaluate arguments only once. For this reason, call-by-need evaluation is used in modern functional programming languages such as Haskell.

\pagebreak
\section{Methodology}

\subsection{Project Aims} % Interpreter, Compiler, & if time permits, a JIT Compiler (Mention JRA project)
As stated, the aim of this project is to implement a functional programming language, based on the lambda calculus. The main feature goals are:
\begin{itemize}
    \item \textbf{Rich Syntax} - An expressive syntax that allows the user to write terse programs, with useful features such as lists, tuples, string literals etcetera.
    \item \textbf{Strong Typing and Type Checking} - A strong typing system with type checking to guarantee that all valid programs exhibit no runtime type errors.
    \item \textbf{Type Inference} - A system for inferring the types of variables without the programmer writing them down.
    \item \textbf{Polymorphic Types} - A typing system that includes polymorphic types, allowing the programmer to write highly generic, reusable code.
    \item \textbf{Interpreter} - An interpreted implementation of the language, to run on any system for which their exists a Haskell implementation.
    \item \textbf{Compiler} - A compiled implementation of the language, targeting GNU/Linux operating systems that run on the x86 CPU architecture.
    \item \textbf{User Documentation} - Detailed, well-written user documentation
\end{itemize}
The following will also be implemented should time permit it:
\begin{itemize}
    \item \textbf{Algebraic Data Types} - Abstract, high-level data types that allow the programmer to encode their problem space in a data structure that is easy to manipulate programmatically.
    \item \textbf{Pattern Matching} - A feature that allows functions to be implemented in different ways for differently structured inputs.
    \item \textbf{JIT Compiler} - A tracing just-in-time system that works begins with interpretation, collecting execution statistics, and when a given function is executed a certain number of times, it is then JIT-compiled.
    \item \textbf{Module System} - A system that allows programmers to break code into separate files, give names to modules, etcetera.
\end{itemize}

\subsection{Scientific Investigation}
Since this project is in a highly abstract, scientific topic, rather than being a traditional software engineering project, a typical requirements analysis is inappropriate. Due to the well established structure of compilers, typical design documentation is also not entirely appropriate - the produced software system will be modularised according to the phases of a compiler that have been explained in the background section of this document.

\subsection{Ethical Issues} % BCS Code of conduct
The British Computer Society (BCS) defines a code of conduct for IT professionals. However, due to the abstract nature of this project, the risk of breaching any of the guidelines in the BCS code of conduct is negligible. Therefore no special measures need be taken to ensure that the project is conducted within those guidelines.

\subsection{Toolchain}
\subsubsection{Arch GNU/Linux}
Arch GNU/Linux is an appropriate development platform that will be used for this project. It has a wide range of development tools that will be required, such as GNU make, a tool for build and test automation, and Vim, a text editor, easily and freely available via a package manager.

\subsubsection{Haskell}
Haskell is an expressive, functional, high level programming language that is ideally suited to programming language implementation. It also has an associated package manager, Cabal, which makes the installation of libraries that will be required for the project very quick and simple.

\subsubsection{Lexer and Parser Generators}
To simplify lexical and syntactic analysis, lexer and parser generators will be used. These will give the project greater flexibility by allowing the syntax of the language to change without having to do major rewrites to the code, but simply make changes to lexical and syntactical specifications for the lexer and parser generators. \textbf{Alex} is a lexer generator for Haskell that will be used, and \textbf{Happy} is the parser generator. They are both freely available as part of the Cabal system.

\subsubsection{Version Control}
Version control is a vital part of any software project, and this one is no exception. Easier recovery from implementation mistakes, backup and change tracking are just a few of the many benefits that version control provides to a software project. The Mercurial version control system will be used for this project. A local copy of the project will be kept, along with a master repository that will be hosted on bitbucket, to prevent loss and allow work to be carried out on multiple machines with little additional effort. \\

\subsubsection{Testing}
Testing is a critical part of any software project. It enables confident refactoring, since any changes to code that prohibit the software system from functioning correctly, are immediately identified. Randomised testing is also very powerful, and helps to discover bugs and 'corner cases' that would otherwise be incredibly difficult to find. The HUnit library will be used for unit-testing in this project.

\pagebreak
\section{Project Plan}
\subsection{Task Dependencies}
The graph below specifies the interdependencies of the subtasks of the project:
\\\\
\centerline{\includegraphics[width=260px]{task_dependencies}}
\centerline{\textbf{Figure: Subtask Dependencies}}

\subsection{Project Schedule}
Below are the target completion dates for the subtasks of the project:
\\\\
\begin{tabular}{l l}
    \indent \textbf{Task}               & \textbf{Target Completion Date} \\
    \indent Interpreter                 & $14^{th}$ November 2013         \\
    \indent Type-Checker and Inferencer & $14^{th}$ December 2013         \\
    \indent Lexer and Parser            & $21^{st}$ January  2014         \\
    \indent Compiler                    & $14^{th}$ March    2014         \\
    \indent Draft report                & $17^{th}$ April    2014         \\
    \indent User Documentation          & $30^{th}$ April    2014         \\
    \indent Final report                & $12^{th}$ May      2014         \\
\end{tabular}
\\\\
Should any tasks be completed before their deadline, the extra time will be used to implement features specified as additional features, in section 3.1 of this document.

\pagebreak
\section{Design and Implementation}
\subsection{Modules}
The language implementation is divided into several modules, each discussed below.
% For each section describe:
%   - Broad purpose
%   - Key datatypes
%   - Key functions
%   - Implementation issues and solutions to them
\subsubsection{Syntax}
The purpose of the Syntax module is primarily to define the representation of programs in the language, rather than to provide any significant functionality. There are five datatypes used to achieve this: \texttt{Prog}, \texttt{Func}, \texttt{Term}, \texttt{Type} and \texttt{OpType}.
\paragraph{The \texttt{Prog} Datatype \\}
This is used to represent programs at the highest level, and is essentially a mapping between function names and their values.
\paragraph{The \texttt{Func} Datatype \\}
This is used to represent functions, and consists of four members - a name (a string) for the function itself, arguments (a list of names), a body (an instance of the Term datatype) and possibly an instance of the Type datatype, representing the type signature of the function. This may not be present since it can be inferred.
\paragraph{The \texttt{Term} Datatype \\}
This is a recursive datatype that defines the bodies of functions. Values of this type may be simple constants, such as integers, characters or booleans, they may be variables, operators such as '+' and '-', lambda expressions, function calls, list literals, sum type literals or product type literals.
\paragraph{The \texttt{Type} Datatype \\}
This is used to represent types of functions and terms. Values of this type may be simple types such as \texttt{Int} (integer), \texttt{Char} (character) or \texttt{Bool} (boolean). They may also be function types, sum types, product types, list types or type variables.
\paragraph{The \texttt{OpType} Datatype \\}
This is used to represent functions that are built into the language, and is essentially just an extension of the Term datatype.
\\\\ A more detailed description of these datatypes is given by the context-free grammar for the language, which can be found in the Language Specification section of this document.
\subsubsection{Interpreter}
The Interpreter module contains functions required for interpreting programs, the most notable of which is \texttt{reduce}, which reduces Terms to their normal form. \texttt{reduce} has the type signature: \texttt{Prog -> Term -> Term}. It takes two arguments, the first of which is of type Prog, which provides a 'context' for Term reduction, that is, the function definitions to which the second parameter, the Term to reduce, may refer. The result is the normal form of the given Term. This function implements the reduction rules for the lambda calculus with additional reduction rules for the additional features of the language. The other functions present in the Interpreter module are used to handle the \emph{name clash} problem, described below.
\\\indent The main difficulty in implementing the interpreter for the language has been the problem of name clashes. When function application is evaluated, one expression (the \emph{argument expression}) is substituted into another (the \emph{body expression}). If the argument expression contains free variables, the names of which are bound in the body expression, those free variables become erroneously captured by the bindings present in the body expression.
\\\indent To handle this problem, $\alpha$ conversion is used. $\alpha$ conversion is the transformation from one expression to another, that is $\alpha$ equivalent (equivalent modulo the choice of variable names) to the original expression. Before substitution, the free variables of the argument expression are computed. If any of those variables are bound in the body expression, those bindings and corresponding variables are renamed to names not present in the argument expression - that is, the body expression is $\alpha$ converted, such that it binds no variables that are free in the argument expression. The argument expression can then be safely substituted into the body expression.
\subsubsection{Unifier}
The Unifier module implements polymorphic type inference, using the Hindley-Milner type inference algorithm (Pierce, 2002:326). This algorithm consists of two phases, a \textbf{constraint generation} phase and a \textbf{unification} phase.
\\\indent Constraint generation involves walking the abstract syntax tree, collecting constraints, which are equations concerning the types in the AST. Consider the expression:
\\\\
\indent \texttt{f 10}
\\\\
Here the function \texttt{f} is applied to a single integer argument, \texttt{10}. This tells us that the type of argument that \texttt{f} takes is integer. This is reflected in the constraint generated for this expression, which can be written formally as 
\\\\
\indent \texttt{T\_f = Int -> T\_a}
\\\\
where \texttt{T\_a} is an unknown type, and the return type of \texttt{f}. The expression \texttt{f 10} might exist within the larger expression:
\\\\
\indent \texttt{not (f 10)}
\\\\
From which we can see that the expression \texttt{f 10} must have type boolean, as it is used as an argument to the function \texttt{not}, which has type \texttt{Bool -> Bool}. This would be reflected in the constraint generated for this conditional expression, which would be
\\\\
\indent \texttt{T\_a = Bool}
\\\\
With the two constraints that we have, we can later construct the complete type of \texttt{f}, which is \texttt{Int -> Bool}, during the unification phase.
\\\indent Unification is the process of satisfying a set of equations, and finds applications in many fields, such as in logic programming languages, where unification is used as the execution mechanism. Here we use unification to find a solution to our set of constraints, that is, `\emph{to check that the set of solutions is nonempty and, if so, to find a ``best'' element, in the sense that all solutions can be generated straightforwardly from this one.}' (Pierce, 2002:326). In the above case, we have the constraints:
\\\\
\indent \texttt{T\_a = Bool} \\
\indent \texttt{T\_f = Int -> T\_a}
\\\\
to which we can straightforwardly find the solution:
\\\\
\indent \texttt{T\_f = Int -> Bool}
\\\\
This is a simple case, however the unification algorithm (given below) will find a solution to an arbitrarily large set of constraints, with arbitrary types in the grammar of types for the language. Where no solution exists, the algorithm will terminate and indicate this, which tells us that the corresponding program is ill-typed. If a solution exists, the algorithm returns a function that can be used to rewrite type variables in the program as the concrete types that they equate to. This function can be thought of as the solution to the constraint set.
\pagebreak
\begin{algorithm}{Unify}{C}
    \begin{IF}{C = \emptyset}
        \lambda x . x
    \ELSE
        \{S = T\} \= \CALL{ArbitraryElement}(C) \\
        C' \= C \setminus \{S = T\} \\
        \begin{SWITCH}
        \item{S = T} \\
            \CALL{Unify}(C')
        \item{S = X \hspace{1mm} \wedge \hspace{1mm} X \not\in \CALL{FV}(T)} \\
            \CALL{Unify}([X \mapsto T]C') \circ [X \mapsto T]
        \item{T = X \hspace{1mm} \wedge \hspace{1mm} X \not\in \CALL{FV}(S)} \\
            \CALL{Unify}([X \mapsto S]C') \circ [X \mapsto S]
        \item{S = S_1 \rightarrow S_2 \hspace{1mm} \wedge \hspace{1mm} T = T_1 \rightarrow T_2} \\
            \CALL{Unify}(C' \cup \{S_1 = T_1, S_2 = T_2\})
        \item{S = list \hspace{1mm} S' \hspace{1mm} \wedge \hspace{1mm} T = list \hspace{1mm} T'} \\
            \CALL{Unify}(C' \cup \{S' = T'\})
        \item{S = product \hspace{1mm} S_1, S_2 \hspace{1mm} \wedge \hspace{1mm} T = product \hspace{1mm} T_1, T_2} \\
            \CALL{Unify}(C' \cup \{S_1 = T_1, S_2 = T_2\})
        \item{S = sum \hspace{1mm} S_1, S_2 \hspace{1mm} \wedge \hspace{1mm} T = sum \hspace{1mm} T_1, T_2} \\
            \CALL{Unify}(C' \cup \{S_1 = T_1, S_2 = T_2\})
        \item{\DEFAULT} \\
            fail
        \end{SWITCH}
    \end{IF}
\end{algorithm}

A problem with this system of constraint generation and unification is that if we have a polymorphic function, such as \texttt{length : $\forall$a.[a] -> Int}, which computes the length of a list, and we apply \texttt{length} to a list of integers, we will then have the constraint that the argument type of \texttt{length} is a list of integers, rather than some arbitrary type. This means that if we were to use the length function elsewhere in the program, on, for example, a list of booleans, we would also have the constraint that the argument type of \texttt{length} is a list of booleans. These two constraints are clearly unsatisfiable and make the whole program untypable. To solve this problem, we need to change the way we generate constraints. When we find a function application, we first reduce the application of the function one step, which removes the constraint that our function's argument type is the type of the argument to which it is being applied in this specific instance. We still, however, ensure that the function is used only in type-safe ways. Use of this strategy gives us a type of polymorphism known as \textbf{Let-Polymorphism} (Pierce, 2002:331).

\subsubsection{Lexer}
The Lexer module implements lexical analysis on source code. This module is generated using a lexical specification and \textbf{Alex}, a lexical analyser generator for Haskell. The lexical specification contains a list of regular expressions to match against source code, each of which has a corresponding token that should be returned when the source code matches that regular expression. The output is a list of tokens, each of which is labelled with its coordinates in the source file, so that when lexical and syntactic errors occur, the location in the source file where the error occurs can be indicated to the user.
\subsubsection{Parser}
The Parser module has the task of assembling the list of tokens from the Lexer, into a ParseResult, which will later be converted into an AST by the PostParsing module. A ParseResult contains two members, a map from function names to function values, and a list of type declarations, which state the types of the functions in the map.
\\\indent The Parser module is generated using a context-free grammar for the language, given in special syntax, which \textbf{Happy}, a parser generator for Haskell, uses to produce a module that parses the language specified in the given grammar.
\subsubsection{PostParsing}
The PostParsing module has two responsibilities:
\begin{enumerate}
    \item[1.] Incorporating the type declarations in the ParseResult into the functions within the map in the ParseResult.
\end{enumerate}
\indent The parser does not enforce a relationship between the location of a function in a file and the location of the corresponding type definition. Therefore it is not possible to return just a map of function names to function values where the functions already have type labels. This is why a ParseResult is returned by the parser, as opposed to an instance of the \texttt{Prog} datatype (which contains only a map from function names to function values).
\begin{enumerate}
    \item[2.] Convert the user's type variable names into integer type variable names.
\end{enumerate}
\indent The language allows the user to supply type variable names as strings, for readability purposes. However these can be slow and messy to handle during type checking, so the strings are converted to integers, which are faster to handle.

\subsubsection{CodeGen}
The CodeGen module generates C code for a given program. The implementation strategy for the generated source code is a simple template instantiator machine. Using this strategy, each function in the source code is compiled into a C function that builds a \emph{template} (essentially just an AST) on the heap. The generated file is then compiled and linked with a C source file (\texttt{langdefs.c}) and a header file (\texttt{langdefs.h}) to produce a binary file. The file \texttt{langdefs.c} contains functions that the generated code uses to compute the result of the program by evaluating the template representation of the main function.
\\\indent This implementation of the language provides many advantages over the interpreted implementation. These include:
\begin{itemize}
    \item Lexing, parsing and type checking is done at compile-time and does not need to be done at runtime. The interpreted implementation must perform these steps each time a program is run.
    \item Compiling to C makes the language highly cross-platform, since there exists a C compiler for virtually every CPU architecture in existence. The only caveat here is that the compilation step must be done on a machine for which there exists an implementation of the Haskell language.
    \item C is known for its speed. The GCC compiler performs many effective optimisations. As a result, evaluating templates in C is faster that interpreting ASTs in Haskell.
    \item Variable names are represented as integers instead of strings. Therefore determining if an abstraction binds a particular variable can be done with a simple integer comparison, which is a single machine instruction, rather than checking to see that each character in the strings match.
    \item The De Bruijn representation of lambda terms is used. In this representation, variables do not store a name, but a number, which is the number of abstractions between the variable itself and the abstraction that binds it. Abstractions themselves need not carry the names of variables in this case. The advantage of this is that the name clash problem does not exist, which removes the overhead associated with checking for and handling name clashes.
\end{itemize}
The traditional method of functional language compilation is closure conversion. In this scheme, local variable definitions are translated into global ones, and lambda expressions are represented as closures. Closures are data structures that contain (via pointers) some code that executes the body of the lambda expression, and a means of accessing variables bound elsewhere. There are many advantages to this implementation strategy, the main one being speed. Programs compiled in this manner can rely much more heavily on machine instructions operating on registers, which are very fast. Schemes that rely on expression reduction, including template instantiation, rely more on machine instructions that use indirect addressing, which are typically much slower.
\\\indent Template instantiation was chosen as the implementation strategy due to the time constraints on the project. Implementing template instantiation is simpler than implementing closure conversion and was therefore was a more realistic target with the time available. One benefit of this choice, however, is that call-by-name evaluation is much easier to implement with template instantiation than with closure conversion - It would not have been possible to implement call-by-name evaluation using closure conversion without a significant amount of extra available time, whereas choosing template instantiation has allowed for the compiled implementation to use call-by-name evaluation, enabling the use of infinite data structures.
\\\indent The initial goal was to implement a template instantiator machine that would evaluate templates \textbf{in-place}, i.e. perform reduction to a template that is in memory. This proved difficult to implement, and due to time constraints, a simpler approach had to be adopted. The simpler approach operates by computing the reduced value of the template in memory, instantiating a new template in memory that is $\beta$-equivalent in one or more reduction steps, to the original template. The original template is then freed from memory and reduction proceeds on the newly created template. This strategy has a higher overhead than in-place reduction, since an entire template must be created and freed in memory with each reduction step, but relies less on low-level memory access and was therefore easier to implement to a working standard.
\subsubsection{Test Modules}
Unit test modules are named according to the module which they test. For example, the tests of the Unifier module are contained in the TestUnifier module. The Syntax, Unifier and Interpreter modules have reasonable test coverage, whereas there is are no tests for the CodeGen module, again due to time constraints.

\subsection{Source Repository}
The structure of source repository, \texttt{fyp} (an initialism for \textbf{f}inal \textbf{y}ear \textbf{p}roject), is explained below:
\\\\
\begin{tabular}{ p{37mm} p{2mm} p{70mm} }
    \texttt{fyp/}                  & - & This is the root directory of the repository. It contains the file \texttt{readme}, which contains a line of text identifying the repository, and two directories, \texttt{src} and \texttt{doc}. \\
                                   &   & \\
    \texttt{fyp/doc}               & - & This directory contains latex and graphviz source files that were used to produce the interim and final reports for this project. There is also a makefile, which when run, produces image files for the final report document, using the \textbf{dot} graph generation tool, and a PDF file using \textbf{pdflatex}. \\
                                   &   & \\
    \texttt{fyp/src}               & - & This directory contains three directories, each of which contains a stand-alone implementation of a language.\\
                                   &   & \\
    \texttt{fyp/src/PureCalculus}  & - & This directory contains an implementation of the 'pure' lambda calculus, that is, a calculus containing only lambda abstractions, function application, and variables. It has no front-end and must be interacted with using the Haskell REPL. \\
                                   &   & \\
    \texttt{fyp/src/TypedCalculus} & - & This directory contains an implementation of the simply typed lambda calculus. This implementation contains, in addition to the features of the pure implementation, integer and boolean constants and arithmetic operations, and the available types are integer, boolean and function types. This implementation uses a simple type checking algorithm (Pierce, 2002:115). Again it has no front-end and must be interacted with using the Haskell REPL. \\\\
                                   &   & \\
    \texttt{fyp/src/Polymorphic}   & - & This directory contains the full language implementation described in this report, and includes the modules described in the previous section. \\
\end{tabular}

\pagebreak
\section{Language Specification}
This section will give details of the implemented language and serve as user documentation.
\subsection{Context-Free Grammar}
The context-free grammar (in BNF) for the language is given below:
\begin{verbatim}
<prog> ::= "type" <identifier-lower-case> "=" <ty> <prog>
    | "def" <identifier-lower-case> <args> "=" <exp> <prog>
    | epsilon

<args> ::= <identifier-lower-case> <args>
    | epsilon

<ty> ::= "Int"
   | "Bool"
   | "Char"
   | "String"
   | "[" <ty> "]"
   | "{" <ty> "|" <ty> "}"
   | "{" <ty> "&" <ty> "}"
   | <ty> "->" <ty>
   | "(" <ty> ")"
   | <identifier-any-case>

<exp> ::= "let" <identifier-lower-case> <args> "=" <exp> "in" <exp>
    | "Î»" <identifier-lower-case> "." <exp>
    | "^" <identifier-lower-case> "." <exp>
    | "\" <identifier-lower-case> "." <exp>
    | <exp> <exp>
    | <identifier-lower-case>
    | "if" <exp> "then" <exp> "else" <exp>
    | <exp> <infix-binary-op> <exp>
    | <unary-op>
    | "(" <exp> ")"
    | "{" <exp> "," "_" "}"
    | "{" "_" "," <exp> "}"
    | "{" <exp> "," <exp> "}"
    | "true"
    | "false"
    | <integer-literal>
    | <string-literal>
    | <list>

<unary-op> ::= "iszero"
    | "not"
    | "reml"
    | "remr"
    | "fst"
    | "snd"
    | "head"
    | "tail"
    | "null"

<list> ::= "[]"
    | "[" <exp> <list-rest>

<list-rest> ::= "," <exp> <list-rest>
    | "]"

<infix-binary-op> ::= "+"
    | "-"
    | "*"
    | "/"
    | "%"
    | "and"
    | "or"
    | "xor"
    | ">"
    | ">="
    | "<"
    | "<="
    | "=="
    | "/="
    | ":"
\end{verbatim}

\subsection{Basic Data Types}
The language has 3 familiar basic data types, which are integers, characters and booleans. In the syntax of the language, these types are denoted as \texttt{Int}, \texttt{Char} and \texttt{Bool} respectively.
\subsection{Functions for Basic Data Types}
The language has a familiar set of built-in functions over the basic data types, which are:
\\\\
\centerline{\begin{tabular}{ | p{13mm} | l | l | l | p{25mm} | }
    \hline
    Function Syntax & Description                       & Type                                  & Example usage             & Evaluated result of example   \\
    \hline
    \texttt{==}     & Equality                          & \texttt{$\forall$a. a -> a -> Bool}   & \texttt{3 == 4}           & \texttt{false}                \\
    \texttt{>}      & Greater than                      & \texttt{Int -> Int -> Bool}           & \texttt{3 > 4}            & \texttt{false}                \\
    \texttt{>=}     & Greater than or equal             & \texttt{Int -> Int -> Bool}           & \texttt{3 >= 4}           & \texttt{false}                \\
    \texttt{<}      & Less than                         & \texttt{Int -> Int -> Bool}           & \texttt{3 < 4}            & \texttt{true}                 \\
    \texttt{<=}     & Less than or equal                & \texttt{Int -> Int -> Bool}           & \texttt{3 <= 4}           & \texttt{true}                 \\
    \texttt{/=}     & Inequality                        & \texttt{Int -> Int -> Bool}           & \texttt{3 == 4}           & \texttt{false}                \\
    \texttt{iszero} & Test for zero                     & \texttt{Int -> Bool}                  & \texttt{iszero 0}         & \texttt{true}                 \\
    \texttt{and}    & Logical conjunction               & \texttt{Bool -> Bool -> Bool}         & \texttt{true and false}   & \texttt{false}                \\
    \texttt{or}     & Logical disjunction               & \texttt{Bool -> Bool -> Bool}         & \texttt{true or false}    & \texttt{true}                 \\
    \texttt{not}    & Logical negation                  & \texttt{Bool -> Bool}                 & \texttt{not false}        & \texttt{true}                 \\
    \texttt{+}      & Binary sum                        & \texttt{Int -> Int -> Int}            & \texttt{1 + 2}            & \texttt{3}                    \\
    \texttt{-}      & Subtraction                       & \texttt{Int -> Int -> Int}            & \texttt{2 * 2}            & \texttt{4}                    \\
    \texttt{*}      & Multiplucation                    & \texttt{Int -> Int -> Int}            & \texttt{3 - 2}            & \texttt{1}                    \\
    \texttt{/}      & Integer division                  & \texttt{Int -> Int -> Int}            & \texttt{10 / 2}           & \texttt{5}                    \\
    \texttt{\%}     & Remainder from integer division   & \texttt{Int -> Int -> Int}            & \texttt{18 \% 4}          & \texttt{2}                    \\
    \hline
\end{tabular}}
\\\\
Not included in this table is the conditional function, which takes the form:
\\\\
\indent \texttt{if G then E else F}
\\\\
This function has the type \texttt{$\forall$a. Bool -> a -> a -> a}, and if the expression \texttt{G} evaluates to true, the whole expression evaluates to \texttt{E}, else the whole expression evaluates to \texttt{F}.

\subsection{Abstract Data Types}
\subsubsection{Lists}
The language has several abstract data types, which can be used in a number of ways. The most familiar of these is the list type. These are denoted ``\texttt{[a]}'' where \texttt{a} is some other type. Every element in a list must have the same type as the other elements in that list. The language provides several functions for use with lists, which are \texttt{head}, which takes a list and returns the first element of that list, \texttt{tail}, which takes a list, and returns a list that is the same as the given list but without the 'head' element. Applying \texttt{head} or \texttt{tail} to an empty list is an exceptional condition, and execution is terminated if such an application is evaluated. The \texttt{null} function takes a list and returns \texttt{true} if that list is empty, \texttt{false} otherwise. The language contains a small set of predefined functions that can be made use of, including a \texttt{length} function that computes the length of a list. The definition of \texttt{length} is shown below, which demonstrates the use of the list functions.
\\\\
\indent\textbf{Definition of the }\texttt{length}\textbf{ function}
\begin{verbatim}
    type length = [a] -> Int
    def length =
        let lenAcc acc lst =
            if null lst then
                acc
            else
                (lenAcc (acc + 1) (tail lst))
        in
        (lenAcc 0)
\end{verbatim}

\subsubsection{Sum Types}
Sum types, also known as union types, are included in the language. They are a data structure that contain a single value that may be one of two types. These are useful in computations that may fail, an example of which is division, which will fail if the denominator is zero. Shown below is an example of a division function that accommodates this possibility using sum types.
\\\\
\indent\textbf{Safe division function}
\begin{verbatim}
    # Returns { _ , false } if denominator is zero,
    # or { result , _ } otherwise.
    type safeDiv = Int -> Int -> { Int | Bool }
    def safeDiv a b =
        if iszero b then
            { _ , false }
        else
            { a / b , _ }
\end{verbatim}
There are two constructors for values in the sum type, which are left-injection, and right-injection. The syntax for left-injection is \texttt{\{ E , \_ \}}, and the syntax for right-injection is \texttt{\{ \_ , E \}}, where \texttt{E} is an expression.
\\\indent There are three destructors for values in the sum type, which are \texttt{remr}, \texttt{reml} and \texttt{isleft}. \texttt{remr} retrieves the right-hand value from a sum expression. This function causes an error if there is no right-hand value (i.e. if there is a left-hand value). \texttt{reml} retrieves the left-hand value from a sum expression. This function causes an error if there is no left-hand value (i.e. if there is a right-hand value). To prevent errors with these two functions, the expression to which they are applied should first be checked with the \texttt{isleft} function, which returns \texttt{true} if a left-hand value is present, and \texttt{false} if the right-hand value is present. The example below demonstrates the use of the \texttt{reml}, \texttt{remr} and \texttt{isleft} functions with the \texttt{safeDiv} function shown above.
\begin{verbatim}
    type doSomeCalculation = Int -> Int -> Int -> { Int | Bool }
    def doSomeCalculation a b c =
        if isleft (safeDiv a b) then
            { (reml (safeDiv a b)) + c , _ }
        else
            { _ , false }
\end{verbatim}
\subsubsection{Product Types}
Product types, also known as tuples, are data structures used to store multiple values of differing types. In this language, product types are binary, but binary product types can be used to encode product types of an arbitrary number of members.
\\\indent There is a single constructor for values in the product type, which takes two values and returns those values in a tuple. The syntax for this is \texttt{\{ E , F \}}, where E and F are expressions. There are two destructors for values in the product type, which are \texttt{fst} and \texttt{snd}, which retrieve the first and second values in a tuple, respectively. The example below demonstrates the use of the product type constructor and the \texttt{fst} and \texttt{snd} functions:
\begin{verbatim}
    type oneAndTrue = { Int & Bool }
    def oneAndTrue = { 1 , true }

    type one = Int
    def one = fst oneAndTrue # Evaluates to 1

    type tru = Bool
    def tru = snd oneAndTrue # Evaluates to true
\end{verbatim}
\subsection{Summary of Functions for Abstract Data Types}
The table below provides a summary of the functions over abstract data types:
\\\\
\centerline{\begin{tabular}{ | p{17mm} | l | l | l | p{25mm} | }
    \hline
    Function Syntax         & Description                       & Type                                              & Example usage                     & Evaluated result of example   \\
    \hline
    \texttt{:}              & List constructor                  & \texttt{$\forall$a. a -> [a] -> [a]}              & \texttt{1:[2, 3]}                 & \texttt{[1, 2, 3]}            \\
    \texttt{[E]}            & Alternative list constructor      & \texttt{$\forall$a. a -> [a]}                     & \texttt{[2]}                      & \texttt{[2]}                  \\
    \texttt{[]}             & Empty list constructor            & \texttt{$\forall$a. [a]}                          & \texttt{[]}                       & \texttt{[]}                   \\
    \texttt{head}           & Head of a list                    & \texttt{$\forall$a. [a] -> a}                     & \texttt{head [2, 3]}              & \texttt{2}                    \\
    \texttt{tail}           & Tail of a list                    & \texttt{$\forall$a. [a] -> [a]}                   & \texttt{tail [1, 2, 3]}           & \texttt{[2, 3]}               \\
    \texttt{null}           & Is-empty function                 & \texttt{$\forall$a. [a] -> Bool}                  & \texttt{null []}                  & \texttt{true}                 \\
    \texttt{\{ E , \_ \}}   & Left-injection sum constructor    & \texttt{$\forall$a, b. a -> \{ a | b \}}          & \texttt{\{ "Q" , \_ \}}           & \texttt{\{ "Q" , \_ \}}       \\
    \texttt{\{ \_ , E \}}   & Right-injection sum constructor   & \texttt{$\forall$a, b. b -> \{ a | b \}}          & \texttt{\{ \_ , 3 \}}             & \texttt{\{ \_ , 3 \}}         \\
    \texttt{reml}           & Remove-left sum destructor        & \texttt{$\forall$a, b. \{ a | b \} -> a}          & \texttt{reml \{ 3 , \_ \}}        & \texttt{3}                    \\
    \texttt{remr}           & Remove-right sum destructor       & \texttt{$\forall$a, b. \{ a | b \} -> b}          & \texttt{remr \{ \_ , 3 \}}        & \texttt{3}                    \\
    \texttt{isleft}         & Is-left sum function              & \texttt{$\forall$a, b. \{ a | b \} -> Bool}       & \texttt{isleft \{ \_ , "\$" \}}   & \texttt{false}                \\
    \texttt{\{ E , F \}}    & Tuple constructor                 & \texttt{$\forall$a, b. a -> b -> \{ a \& b \}}    & \texttt{\{ 2 , true \}}           & \texttt{\{ 2 , true \}}       \\
    \texttt{fst}            & First tuple destructor            & \texttt{$\forall$a, b. \{ a \& b \} -> a}         & \texttt{fst \{ 2 , true \}}       & \texttt{2}                    \\
    \texttt{snd}            & Second tuple destructor           & \texttt{$\forall$a, b. \{ a \& b \} -> b}         & \texttt{snd \{ 2 , true \}}       & \texttt{true}                 \\
    \hline
\end{tabular}}
\subsection{Function Definitions}
Global function definitions are made using the \texttt{def} keyword, followed by the name of the function and the name of each argument, and an '\texttt{=}' sign, after which the function body is included. They are mutually recursive, and globally visible, except within abstractions that bind the same name as global functions. Several examples are provided above.
\\\indent The language also allows local function definitions, visible only from the function in which they are defined. They can be defined either using \texttt{let} notation, or using lambda notation. Below is an example of a local function definition using \texttt{let} notation:
\begin{verbatim}
    def doSomeCalculation a b c d =
        let sumThree a b c = a + b + c in
            ((sumThree b c d) * a)
\end{verbatim}
Lambda notation can also be used to define the same function. In fact, the \texttt{let} notation is just syntactic sugar for the lambda notation:
\begin{verbatim}
    def doSomeCalculation a b c d =
            (\sumThree.((sumThree b c d) * a))(\a.\b.\c. a + b + c)
\end{verbatim}

\subsection{Type Inference and Type Declarations}
The language has type inference, and will type-check programs without being provided with any type declarations. Type declarations can be included for top-level function declarations. These will be checked against the inferred type for the function, and the type check will fail should they differ.
\\\indent While the type declarations are not required by the language, they are useful in several ways. They prevent the programmer from making errors, helping them to understand their code. They serve as a form of documentation. They can also be used to restrict the type of a function that would be inferred as a more general polymorphic type.
\\\indent The context-free grammar at the start of this chapter shows the structure of type declarations. Function types are denoted with an infix arrow: '\texttt{->}', with the argument type on the left and the return type on the right. List types are denoted with square brackets, for example, \texttt{[Int]} represents a list of integers. Sum types are denoted with braces, containing two types separated by a vertical bar: \texttt{\{ Int | Bool \}} represents a type that is the sum of the integer and boolean types. Product types are dentoted similarly to sum types - the vertical bar is exchanged for the ampersand symbol: \texttt{\{ Int \& Bool \}} represents a type that is the product of the integer and boolean types. The choice of syntax for sum and product type declarations was made because it in some sense illustrates the meaning of the construct - the vertical bar is often used to represent the logical 'or' function, and this may help illustrate to programmers that a value in a sum type is a value of either type \texttt{a} \textbf{or} type \texttt{b}. Conversely, the ampersand symbol is often used to represent the logical 'and' function, and again may help communicate to the programmer that a value in a product type contains values of type \texttt{a} \textbf{and} type \texttt{b}.
\subsection{Prelude Functions}
The language has a small prelude of predefined functions. These are:
\begin{itemize}
    \item \texttt{pow} - raise the first argument to the power of the second. Evaluates to \texttt{0} if the second argument is negative.
    \item \texttt{length} - Compute the length of a list.
    \item \texttt{parseInt} - Parse an integer from a string. Non-digit characters are treated the same as the '\texttt{0}' character, so, for example, \texttt{parseInt "123a"} will evaluate to \texttt{1230}.
    \item \texttt{reverse} - Reverse a list such that the last element becomes the first, the penultimate element becomes the second, etcetera.
    \item \texttt{concat} - Concatenate two lists.
    \item \texttt{map} - Apply the given function to each element in a list.
    \item \texttt{zip} - 'Zip up' two lists in to a list of tuples. If one list is longer than the other, trailing elements are dropped.
\end{itemize}
The prelude functions are summarised in the table below:
\\\\
\centerline{\begin{tabular}{ | p{17mm} | p{25mm} | l | l | p{25mm} | }
    \hline
    Function Syntax     & Description                       & Type                                              & Example usage                                 & Evaluated result of example   \\
    \hline
    \texttt{pow}        & Exponentiation                    & \texttt{Int -> Int -> Int}                        & \texttt{pow 2 8}                              & \texttt{256}                  \\
    \texttt{length}     & Length of a list                  & \texttt{$\forall$a. [a] -> Int}                   & \texttt{length ["a", "b", "c"]}               & \texttt{3}                    \\
    \texttt{parseInt}   & Parse an integer from a string    & \texttt{String -> Int}                            & \texttt{parseInt "1234"}                      & \texttt{1234}                 \\
    \texttt{reverse}    & Reverse a list                    & \texttt{$\forall$a. [a] -> [a]}                   & \texttt{reverse [1, 3, 4]}                    & \texttt{[4, 3, 1]}            \\
    \texttt{concat}     & Concatenate two lists             & \texttt{$\forall$a. [a] -> [a] -> [a]}            & \texttt{concat [1, 3] [2, 3]}                 & \texttt{[1, 3, 2, 3]}         \\
    \texttt{map}        & Map a function over a list        & \texttt{$\forall$a, b. (a -> b) -> [a] -> [b]}    & \texttt{map ($\lambda$x.pow x 2) [1, 2, 3]}   & \texttt{[1, 4, 9]}            \\
    \texttt{zip}        & Zip two lists into a list of tuples & \texttt{$\forall$a, b. [a] -> [b] -> \{ a \& b \}} & \texttt{zip [1, 2] ["a", "b"]}              & \texttt{[\{ 1 , "a" \}, \{ 2 , "b" \}]}            \\
    \hline
\end{tabular}}
\subsection{Polymorphic Types}
The language has a form of polymorphism known as let-polymorphism (Pierce, 2002:331), implemented using the Hindley-Milner type inference algorithm. Let-polymorphism allows for the definition of functions that can take arguments of different types. This differs from ad-hoc polymorphism (often called overloading) in that the same code is run on arguments of different types, rather than simply allowing functions with the same name but that are differentiated by the types of arguments that they take. Type declarations for polymorphic functions may use type variables, for example, the type of the \texttt{map} function is:
\\\\
\indent \texttt{type map = (a -> b) -> [a] -> [b]}
\\\\
This contains two different type variables, \texttt{a} and \texttt{b}, which communicates that the function operates on values of two types which may be distinct.
\subsection{Command-Line Arguments}
Command line arguments to programs are represented in the language as a predefined function called \texttt{args}, with type \texttt{[String]}. For this reason, top-level functions may not have the name \texttt{args}. The example below is a complete program that parses an integer from the command line, and returns that integer, plus ten:
\begin{verbatim}
    type main = Int
    def main = 10 + (parseInt (head args))
\end{verbatim}
\subsection{Installation and Interface}
A Haskell implementation, such as GHC, must be installed in order to use the language. The following instructions will assume that GHC is the Haskell implementation being used. A C compiler such as GCC must also be installed in order to use the compiled implementation. To use the compiler and interpreter, navigate to the \texttt{src/Polymorphic} directory of the repository. The system can be compiled by running the command:
\\\\
\indent \texttt{ghc --make Main.hs -o Main}
\\\\
Alternatively, if make is installed, just run the command:
\\\\
\indent \texttt{make}
\\\\
The interpreter can then be invoked by running:
\\\\
\indent \texttt{./Main --interpret path/to/sourcefile}
\\\\
Arguments to the program can be included after the filename. The \texttt{--interpret} flag can be replaced with the \texttt{-i} flag. The program will then be evaluated, and the result of that evaluation will be printed to the standard output. Programs can be compiled by running:
\\\\
\indent \texttt{./Main --compile path/to/sourcefile}
\\\\
which will produce a binary file with the same name as the given input file, appended with the text \texttt{"\_binary"}. For example, if the supplied source file had the name \texttt{"hello.prog"}, the output binary file would have the name \texttt{"hello.prog\_binary"}. Again, the \texttt{--compile} flag can be abbreviated to \texttt{-c}. The produced binary can the be run as desired, and any arguments can be supplied to it from the command line at that time. The program will be run and the result will be printed to the standard output.
\pagebreak
\section{Evaluation}
\subsection{Performance}
The produced system is robust, and contains no known bugs. The interpreted implementation runs slowly, particularly when run inside the Haskell interpreter. Performance is better when the interpreter is compiled with GHC. The interpreter is fine for simple programs, however would not be suitable for large programs or programs that require high performance. The compiled implementation provides a significant increase in speed over the interpreted implementation. It is not so fast as to be a viable replacement for a commercial language, however, but such a target was not specified for this project.
\subsection{Shortcomings}
\subsubsection{Compilation to C}
One of the initial goals for the project was to implement compilation of the language. The term 'compiler' usually refers to the translation of high-level language code into low-level language code. The target language for the compiler implemented in this project is C, which is usually not considered to be a low-level language (although it is low-level in comparison to a language like Haskell or the language implemented in this project).
\subsubsection{Template Instantiation}
A template instantiator machine is not in the strictest sense a low-level implementation. The nature of template instantiation is such that the abstraction of the program as an expression to be evaluated, still exists. For the implementation to be considered low-level, the implementation strategy would have to be one that maps more closely to the nature of the hardware on which the language runs, which would ideally constitute the lack of an abstraction such as the aforementioned expression-to-evaluate abstraction, and be imperative in nature.

% Comments on performance and robustness of each feature e.g. interpreter, type inference, etc.
% What I would do with more time
\subsection{Possible Extensions}
\subsubsection{Closure Conversion}
Had time permitted it, compilation via closure conversion would have been the next undertaking for the project. Such a compiler, to a language such as C or x86 assembly would certainly be a valuable addition to the project, and would produce faster code.
\subsubsection{Type Aliases}
Another possible extension would be to implement type aliases. Without type aliases, when type declarations are included, they have to be written 'in full' - if the user works with a data structure composed of list, sum and product types, they have to write down the entire expression describing that type in every place it is used, assuming they include type declarations in their program. This can be tedious, and a mechanism to avoid this repetition would be desirable. Such a mechanism would allow the user to declare a single word name for a type expression, such that the name could be included in place of the expression wherever it would otherwise have to be written in full.
\subsubsection{Interactive REPL}
An interactive REPL (Read-Evaluate-Print Loop) would be a useful extension to the project. This would allow the user to enter an interactive environment, where they could type expressions in the language and see the result of evaluating those expressions immediately. It might also allow them to make function declarations, to which subsequent expressions could refer.
\subsubsection{Pattern Matching}
Another useful extension would be to implement pattern matching. At present, to determine the structure of a variable, the user must write conditional expressions to achieve this. Pattern matching syntax, such as Haskell's \texttt{case} expressions, are an elegant solution to this problem.

\subsection{Concluding Remarks}
The project undertaken was certainly an ambitious one. The concepts involved are not straightforward to understand. A large amount of background reading and study was required to complete it, particularly to implement the type system of the language. All the goals for the project have been achieved, and although the compiled implementation is somewhat unsophisticated, it is functional.

\pagebreak
\section{Appendix}
\subsection{References}
\begin{itemize}
    \item Types and Programming Languages - Benjamin C. Pierce
    \item Functional Programming - Anthony J. Field and Peter G. Harrison
    \item A Tutorial Introduction to the Lambda Calculus - Ra\'{u}l Rojas \\
        http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf
    \item Modern Compiler Implementation in ML - Andrew W. Appel
    \item Implementing Functional Languages: A Tutorial - Simon Peyton-Jones and David Lester
\end{itemize}

\end{document}

% Final year project - Final report
% Author: Alexander Jeffery

\documentclass{article}

% Package imports:
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\DeclareGraphicsExtensions{.png}

\title{\textbf{Final Report}: An Interpreter and Compiler for a Functional Programming Language}
\author{Alexander Jeffery\\
Candidate Number: 18512\\
Supervisor: Dr. Martin Berger}

\begin{document}
\maketitle

\section{Introduction}

Conventional programming languages are designed to solve problems by decomposing them into a list of instructions, which are executed by a computer in a specified sequence. They are said to be \emph{imperative}. This imperative principle of sequential execution is ultimately derived from the nature of computer architecture, which is imperative in nature. Such languages have become increasingly sophisticated over many years, abstracting the computer's hardware, so that programmers need not concern themselves with understanding the intricate details of the computer, which are often irrelevant to the problem they are solving. Imperative languages do not, however, abstract sequential execution (by definition).
\\
\indent In contrast, Functional programming languages differ in that they are derived from the $\lambda$ calculus, rather than computer architecture, and \emph{do} abstract the imperative nature of computer architecture. This report will explain in detail how computation is achieved in a functional programming language, and give details of the implementation of the functional programming language that is the focus of this project.

\pagebreak
\section{Background}

\subsection{Programming Language Implementation}
Even those unfamiliar with computers may have heard it said that 'computers only understand binary'. In a very specific sense, this is true - CPUs take their instructions in binary form, as voltages on a wire. Therefore we must be able to \emph{express} our computer programs in binary form (such a form is typically referred to as \emph{machine code}). This does not mean, however, that we must write our computer programs in machine code - modern computer programs are created with programming languages, which describe programs in a more abstract way. Typically, \emph{source code} is written in a particular programming language, and is then translated into an executable form via a process known as \emph{compilation}. A compiler is a computer program that takes source code as its input, and (typically) outputs an executable machine code file. Not all programming languages are compiled, however. Some programming languages are \emph{interpreted}, and some languages use a hybrid system of compilation and interpretation, usually to improve performance. The implementation of a typical programming language can be decomposed into several phases: 
\begin{enumerate}
    \item Lexical Analysis
    \item Syntax Analysis
    \item Semantic Analysis
    \item Optimisation
    \item Execution System
\end{enumerate}

\subsubsection{Lexical Analysis}
Lexical analysis is concerned with the identification of words in source code. The input to a lexical analyser, or \emph{lexer}, is a string of characters that (ideally) represents a program. The lexical analyser has the job of \emph{tokenising}, that is, breaking up the words in the string into \emph{tokens}, which are the 'atoms' of the programming language, the simplest units of meaning within the language. The output of a lexical analyser is a list of tokens, assuming all the words in the source code file are valid words in the programming language. If not, this is usually indicated to the user and no further work is done by the compiler.

\subsubsection{Syntax Analysis}
Syntax analysis is concerned with deriving structure from from a sequence of tokens. A syntax analyser, or \emph{parser}, has the job of building an \emph{abstract syntax tree} (AST). An AST is an abstract data structure which represents the structure of a computer program. They are generated from a list of tokens that are the output of the lexer, based on a description of the \emph{grammar} of the language, which is usually represented formally using a \emph{context-free grammar}. An AST, representing the computer program, is the output of the parser, assuming that the tokens that are input can be structured correctly (the correctness of any given structure is specified by the language's context-free grammar). If not, this is usually indicated to the user and no further work is done by the compiler.

\subsubsection{Semantic Analysis}
Semantic Analysis is concerned with ensuring that the semantics of AST, which is output by the parser, are correct. This typically includes \emph{type checking}, which ensures that the programmer made no \emph{type errors} in their program, for example, adding a variable consisting of a number, to a variable consisting of a string of characters. Semantic analysis may also include type \emph{inference}, depending on the programming language. This involves not only type checking, but determining what types variables have without the programmer having to write them down. Semantic analysis will also typically check for errors such as the use of an undefined variable, or the use of a variable prior to its definition.

\subsubsection{Optimisation}
Optimisation is concerned with improving performance of the program. At this point the program has been lexically, syntactically and semantically analysed, we have assurance that the program is well-formed, so the focus is no longer on determining the validity of the program, but rather on its performance. Optimisation takes on many different forms, at many different domains. A typical optimisation might be to generate some \emph{intermediate code} that represents the program at a level that is \emph{closer to the machine}, that looks much more like actual machine code. This intermediate code is then optimised using techniques such as register allocation optimisation, AST transformation and algebraic simplification. Such techniques are beyond the scope of this project, however. The optimised intermediate code can then be transformed into machine code.
\indent The optimisation phase is often skipped with interpreted programming languages, since the focus of interpreted languages tend not to be on performance - often the focus is on portability or some other aspect related to the convenience of the programmer or user.

\subsubsection{Execution Systems}
\paragraph{Interpretation \\}
Interpretation is an execution system that performs \emph{immediate evaluation} of ASTs. Instead of generating machine code to be run at a later time, the program is immediately executed, having just been lexically, syntactically and semantically analysed.

\paragraph{Code Generation \\}
Code generation involves outputting machine code while traversing the AST. The generated machine code is then saved as a file, which can be directly executed by the computer at a later time. Thus, we have a distinction between \emph{compile-time} and \emph{runtime}, that does not exist with interpretation.

\paragraph{Just-In-Time Systems \\}
Just-in-time (JIT) compilation is similar to interpretation in that there is no compile-time/runtime distinction. However, machine code \emph{is} generated with JIT, but instead of it being written to a file, it is mapped directly into the computer's memory for immediate execution. A JIT system will have a program managing the generation and execution of machine code, so that it is as efficient as possible - for example, if some piece of source code is never executed, it would be wasteful to generate machine code for it. For this reason, source code (or an AST, depending on the strategy used by the JIT system) is only translated into machine code at the last possible moment prior to execution (hence the term just-in-time). JIT systems are effective because much more information about the execution of the program is known at runtime as opposed to compile-time, meaning that JIT-compiled code can be optimised to a much greater degree than code compiled ahead of time.

\paragraph{Hybrid Systems \\}
There are many hybrid systems in existence that combine interpretation, code generation and JIT systems into a programming language implementation. One such example is the Java programming language, which generates intermediate code at compile-time, and at runtime, the intermediate code is executed by a \emph{tracing} JIT system, where code is initially interpreted, and when a part of the program is determined to be 'hot code' - code that is executed frequently - it is translated to machine code and directly executed. There are a vast array of different hybrid systems in existence, and many possible combinations that have not been attempted. The goal is usually to maximise performance.

\subsection{Functional \& Imperative}
The $\lambda$ calculus ('$\lambda$' pronounced as 'lambda') is a mathematical calculus that is used to represent computations, and is Turing-complete. It was developed by the mathematician Alonzo Church in 1936 as a proposed solution to the foundational crisis of mathematics. While it was not successful as a foundation for mathematics, it has proved effective as a foundation for functional programming languages.
\\
\indent The $\lambda$ calculus employs a single rule in order to achieve computation, whereby \emph{variables} are replaced with \emph{expressions} that are supplied as \emph{arguments} to \emph{functions}. More precisely, we say that a function is \emph{applied} to its arguments, whereby all occurrences of the parameterised variable in the function body are replaced with the argument expression. For example, consider a function, in a conventional programming language, that takes a single integer argument, and calculates the square of that argument, returning the result. In a C-like language, we might have:
\\\\
\indent \texttt{int square ( int x ) \{ return x * x ; \}}
\\\\
We might invoke this function using:
\\\\
\indent \texttt{square ( 10 ) ;}
\\\\
In the $\lambda$ calculus, we would say that the function \texttt{square} is applied to the argument, \texttt{10}. When a function such as this is applied to an argument in the $\lambda$ calculus, we think of the parameter \texttt{x} taken by \texttt{square} as being \emph{syntactically replaced} in the body of \texttt{square}, by the expression that is supplied as an argument. In this example, such a syntactic replacement would yield
\\\\
\indent \texttt{10 * 10}
\\\\
in the body of \texttt{square}. Such a syntactic replacement can be thought of as the mode of computation for the $\lambda$ calculus. That is, the $\lambda$ calculus achieves computation via this syntactic replacement.
\\
\indent Now let us consider the \texttt{square} function in $\lambda$ notation. We have:
\[ \lambda x.x * x \]
Immediately we see that the function name, \texttt{square}, is no longer present. This is because functions in the $\lambda$ calculus do not have names. This means that in order to use the function, we must write it down in its entirety, rather than refer to it by name like we did in the earlier example. Here, the '$\lambda$' symbol indicates that the expression that follows is a function, and that the '$x$' that follows indicates that $x$ is a parameter to the function. The $\lambda x$ is said to \emph{bind} the variable $x$. The '$.$' is there to delimit this binding of $x$ from the function body, which is the '$x * x$' part. We can apply (invoke) this function to the argument \texttt{10} as we did with the imperative example. In the $\lambda$ calculus, we write:
\[ (\lambda x.x * x)(10) \]
Here, the $10$ parameterises $x$. When we evaluate the expression, we are left with the body of the function, where every occurrence of $x$ is replaced with $10$:
\[ 10 * 10 \]
This syntactic replacement, also called $\beta$-reduction ($\beta$ pronounced as 'beta'), is the sole method of computation in the $\lambda$ calculus. The formal syntax of the $\lambda$ calculus is given by the context-free grammar:
\[ M \:\; ::= \;\; x \;\; | \;\; \lambda x.M \;\; | \;\;  MN \]
where $x$ is an arbitrary variable name and $M$ and $N$ are arbitrary expressions. Closure properties for the $\lambda$ calculus are formalised as:
\[ M \rightarrow N \Rightarrow LM \rightarrow LN. \]
\[ M \rightarrow N \Rightarrow ML \rightarrow NL. \]
\[ M \rightarrow N \Rightarrow \lambda x.M \rightarrow \lambda x.N \]
Informally, we can describe the closure properties: if $M$ reduces to $N$, then $LM$ and $ML$ reduce to $LN$ and $NL$ respectively, where L is an arbitrary expression. $\beta$-reduction is formalised as:
\[ (\lambda x.M)N \rightarrow_\beta M[N/x] \]
\indent As stated previously, functional programming languages are derived from the $\lambda$ calculus, and so their method of computation is this $\beta$-reduction, rather than sequential execution. While they introduce extra syntax for convenience, all functional programs can be converted into $\lambda$ expressions that can be described by the given context-free grammar.

\pagebreak
\section{Methodology}

\subsection{Project Aims} % Interpreter, Compiler, & if time permits, a JIT Compiler (Mention JRA project)
As stated, the aim of this project is to implement a functional programming language, based on the $\lambda$ calculus. The main features I will implement are:
\begin{itemize}
    \item \textbf{Rich Syntax} - An expressive syntax that allows the user to write terse programs, with useful features such as lists, tuples, string literals etcetera.
    \item \textbf{Strong Typing \& Type Checking} - A strong typing system with type checking to guarantee that all valid programs exhibit no runtime type errors.
    \item \textbf{Type Inference} - A system for inferring the types of variables without the programmer writing them down.
    \item \textbf{Polymorphic Types} - A typing system that includes polymorphic types, allowing the programmer to write highly generic, reusable code.
    \item \textbf{Interpreter} - An interpreted implementation of the language, to run on any system for which their exists a Haskell implementation.
    \item \textbf{Compiler} - A compiled implementation of the language, targeting GNU/Linux operating systems that run on the x86 CPU architecture.
    \item \textbf{User Documentation} - Detailed, well-written user documentation
\end{itemize}
Should time permit, I would also like to implement the following additional features:
\begin{itemize}
    \item \textbf{Algebraic Data Types} - Abstract, high-level data types that allow the programmer to encode their problem space in a data structure that is easy to manipulate programmatically.
    \item \textbf{Pattern Matching} - A feature that allows functions to be implemented in different ways for differently structured inputs.
    \item \textbf{JIT Compiler} - A tracing just-in-time system that works begins with interpretation, collecting execution statistics, and when a given function is executed a certain number of times, it is then JIT-compiled.
    \item \textbf{Module System} - A system that allows programmers to break code into separate files, give names to modules, etcetera.
\end{itemize}

\subsection{Scientific Investigation}
Since this project is in a highly abstract, scientific topic, rather than being a traditional software engineering project, a typical requirements analysis is inappropriate. Due to the well established structure of compilers, typical design documentation is also not entirely appropriate - the produced software system will be modularised according to the phases of a compiler that have been explained in the background section of this document.

\subsection{Ethical Issues} % BCS Code of conduct
The British Computer Society (BCS) defines a code of conduct for IT professionals. However, due to the abstract nature of this project, the risk of breaching any of the guidelines in the BCS code of conduct is negligible. Therefore no measures need be taken to ensure that the completion of the project falls within those guidelines.

\subsection{Toolchain}
\subsubsection{Arch GNU/Linux}
Arch GNU/Linux is an appropriate development platform that I will be using for this project. It has a wide range of development tools that I will require, such as GNU make, a tool for build \& test automation, \& Vim, a text editor, easily and freely available via a package manager.

\subsubsection{Haskell}
Haskell is an expressive, functional, high level programming language that is ideally suited to programming language implementation. It also has an associated package manager, Cabal, which makes the installation of libraries that I will require for the project very quick and simple.

\subsubsection{Lexer \& Parser Generators}
To simplify lexical \& syntactic analysis, I will be using lexer \& parser generators. This will give the project greater flexibility by allowing the syntax of the language to change without having to do major rewrites to the code, but simply make changes to lexical \& syntactical specifications for the lexer \& parser generators. \textbf{Alex} is a lexer generator for Haskell that I will be using, and \textbf{Happy} is the parser generator. They are both freely available as part of the Cabal system.

\subsubsection{Testing}
Testing is a critical part of any software project. It enables confident refactoring, since any changes to code that prohibit the software system from functioning correctly, are immediately identified. Randomised testing is also very powerful, and helps to discover bugs and 'corner cases' that would otherwise be incredibly difficult to find.
\\
\indent I will be using the HUnit library to unit-test my code, and also the QuickCheck library for randomised testing. Both are freely available as part of the Cabal system.

\pagebreak
\section{Project Plan}
\subsection{State of the Project} % Problems with Î±-conversion, etcetera.
At the time of writing, an interpreter for $\lambda$ calculus expressions is almost complete, with some unit tests. This has been problematic, as there are many subtleties to $\lambda$ expression evaluation, such as the name clash problem. The name clash problem occurs during $\beta$-reduction - when evaluating an application, if the argument expression contains variable names that occur as bound variables in the body expression, and the substitution is done naively, there is a possibility that the resultant expression becomes incorrect as free names from the argument expression erroneously become bound by abstractions in the body expression. To avoid this problem, I have implemented an \emph{$\alpha$-conversion} function that renames bound variables in the body expression before substitution, so that there is no possibility for name clashes to occur.
\\
\indent There is currently no front-end, that is, the interpreter operates on ASTs, but there currently exists no lexer, parser or semantic analyser, so it is not yet possible to directly run source code. Abstract syntax expressions must be encoded in haskell in order to use the interpreter code.

\subsection{Task Dependencies}
The graph below specifies the interdependencies of the subtasks of the project:
\\\\
\centerline{\includegraphics[width=260px]{task_dependencies}}
\centerline{\textbf{Figure: Subtask Dependencies}}

\subsection{Project Schedule}
Below are the target completion dates for the subtasks of the project:
\\\\
\begin{tabular}{l l}
    \indent \textbf{Task}              & \textbf{Target Completion Date} \\
    \indent Interpreter                & $14^{th}$ November 2013         \\
    \indent Type-Checker \& Inferencer & $14^{th}$ December 2013         \\
    \indent Lexer \& Parser            & $21^{st}$ January  2014         \\
    \indent Compiler                   & $14^{th}$ March    2014         \\
    \indent Draft report               & $17^{th}$ April    2014         \\
    \indent User Documentation         & $30^{th}$ April    2014         \\
    \indent Final report               & $12^{th}$ May      2014         \\
\end{tabular}
\\\\
Should I complete tasks early, the extra time will be used to implement features specified as additional features, in section 3.1 of this document.

\pagebreak
\section{Design \& Implementation}
\subsection{Modules}
The language implementation is divided into several modules discussed below.
% For each section describe:
%   - Broad purpose
%   - Key datatypes
%   - Key functions
%   - Implementation issues and solutions to them
\subsubsection{Syntax}
\subsubsection{Interpreter}
\subsubsection{CodeGen}
\subsubsection{Unifier}
\subsubsection{Lexer}
\subsubsection{Parser}
\subsubsection{PostParsing}
\subsubsection{Test Modules}

\subsection{Testing}
\subsection{Source Repository}
% PureCalculus, TypedCalculus & Polymorphic

\pagebreak
\section{Language Specification}
This section will give details of the implemented language and serve as user documentation.
\subsection{Context-Free Grammar}
\subsection{Basic Data Types}
\subsection{Functions for Basic Data Types}
\subsection{Abstract Data Types}
\subsection{Functions for Abstract Data Types}
\subsection{Function Definitions}
\subsection{Type Declarations}
\subsection{Prelude Functions}
\subsection{Type Inference}
\subsection{Polymorphic Types}
\subsection{Interpreted Implementation}
\subsection{Compiled Implementation}
\subsection{Interface}

\pagebreak
\section{Evaluation}
% Comments on performance and robustness of each feature e.g. interpreter, type inference, etc.
% What has not yet been done & why not
% What I would do with more time

\pagebreak
\section{Appendix}
\subsection{References}
\begin{itemize}
    \item Types \& Programming Languages - Benjamin C. Pierce
    \item Functional Programming - Anthony J. Field \& Peter G. Harrison
    \item A Tutorial Introduction to the Lambda Calculus - Ra\'{u}l Rojas \\
        http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf
\end{itemize}

\subsection{Project Proposal}
\end{document}
